# multi-head-attention
Basic implementation of [the transformer model with multi-head attention](https://arxiv.org/abs/1706.03762) using PyTorch.
