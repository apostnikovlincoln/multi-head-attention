# multi-head-attention
Basic implementation of the transformer model with multi-head attention using PyTorch
